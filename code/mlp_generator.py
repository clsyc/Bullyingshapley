import os
import warnings
import pandas as pd
import shutil

from keras import optimizers
from keras.models import Sequential
from keras.layers import Flatten, Dense, Dropout

import time

from constants import NODES, ACT_FUNCS, MLP_OPTIMIZER, MLP_LEARNING_RATE, MLP_DECAY, MLP_MOMENTUM, MLP_DROPOUT, MLP_LOSS_FUNCTION, MLP_ONE_SHOT

class MLPSearchSpace(object):
    """
    NAS search space of plausible MLP architectures given a vocabulary of unique pairs of a node and activation function. Each pair is mapped to a unique index, whose value should then be sampled by the NAS system to form a valid architecture.
    """
    def __init__(self, num_classes):
        self.target_classes = num_classes
        self.vocab = self.vocab_dict()

    def vocab_dict(self):
        nodes = NODES
        act_funcs = ACT_FUNCS

        # List of (node, act_func) pairs
        layer_params = []

        # List of unique indexes compliant with
        # the above pairs to form a unique mapping
        layer_id = []

        for i in range(len(nodes)):
            for j in range(len(act_funcs)):
                layer_params.append((nodes[i], act_funcs[j]))
                layer_id.append(len(act_funcs) * i + j + 1) 

        # Associate paird to indexes
        vocab = dict(zip(layer_id, layer_params))

        # Add the dropout layer
        vocab[len(vocab) + 1] = (('dropout'))

        # Add the finaly layer accordingly
        if self.target_classes == 2:
            vocab[len(vocab) + 1] = (self.target_classes - 1, 'sigmoid')
        else:
            vocab[len(vocab) + 1] = (self.target_classes, 'softmax')

        return vocab

    def encode_sequence(self, sequence):
        """
        Encode a list of (node, act_func) pairs in a list of indexes.
        """
        keys = list(self.vocab.keys())
        values = list(self.vocab.values())

        encoded_sequence = []

        for value in sequence:
            encoded_sequence.append(keys[values.index(value)])

        return encoded_sequence

    def decode_sequence(self, sequence):
        """
        Decode a list of indexes into a list of (node, act_func) pairs.
        """
        keys = list(self.vocab.keys())
        values = list(self.vocab.values())

        decoded_sequence = []

        for key in sequence:
            decoded_sequence.append(values[keys.index(key)])

        return decoded_sequence

class MLPGenerator(MLPSearchSpace):

    def __init__(self, num_classes):

        self.target_classes = num_classes # Take it from the dataset

        # Load MLP hyper-parameters
        self.mlp_optimizer = MLP_OPTIMIZER
        self.mlp_lr = MLP_LEARNING_RATE
        self.mlp_decay = MLP_DECAY
        self.mlp_momentum = MLP_MOMENTUM
        self.mlp_dropout = MLP_DROPOUT
        self.mlp_loss_func = MLP_LOSS_FUNCTION
        self.mlp_one_shot = MLP_ONE_SHOT

        self.metrics = ['accuracy'] # Add more metrics if needed

        super().__init__(num_classes)

        # Setup a Pandas DataFrame to store share weights between bigrams generated by the NAS across epochs. If a high number of epochs is desired then a smarter solution should be implied, as search times through the dataset every time an architecture is sampled hinder significantly the performance of the model on big datasets.
        if self.mlp_one_shot:
            self.weights_file = 'shared_weights.pkl'
            self.shared_weights = pd.DataFrame({'bigram_id': [],
                                                'weights': []})
            
    def create_model(self, sequence, mlp_input_shape):
        """
        Decode a sampled sequence and create a Keras model.
        """
        layer_configs = self.decode_sequence(sequence)
        model = Sequential()

        if len(mlp_input_shape) > 1:
            # If input is more than 1-dimensional, flatten it...
            model.add(Flatten(name='flatten', input_shape=mlp_input_shape))

            for i, layer_conf in enumerate(layer_configs):
                if layer_conf == 'dropout':
                    model.add(Dropout(self.mlp_dropout, name=f'dropout_{i}'))
                else:
                    model.add(Dense(units=layer_conf[0], activation=layer_conf[1]))
        else:
            for i, layer_conf in enumerate(layer_configs):
                # ...else always make sure it starts with a dense layer
                if i == 0:
                    model.add(Dense(units=layer_conf[0], activation=layer_conf[1],
                                    input_shape=mlp_input_shape))
                elif layer_conf == 'dropout':
                    model.add(Dropout(self.mlp_dropout, name=f'dropout_{i}'))
                else:
                    model.add(Dense(units=layer_conf[0], activation=layer_conf[1]))
        # Return the created Keras model
        return model

    def compile_model(self, model):
        """
        Compile a Keras model with the desired optimizer.
        """
        if self.mlp_optimizer == 'SGD':
            optim = optimizers.SGD(lr=self.mlp_lr, weight_decay=self.mlp_decay,
                                momentum=self.mlp_momentum)
        else:
            optim = getattr(optimizers, self.mlp_optimizer)(learning_rate=self.mlp_lr,
                            weight_decay=self.mlp_decay)
        model.compile(loss=self.mlp_loss_func, optimizer=optim,
                    metrics=self.metrics)
        return model

    def update_weights(self, model):
        """
        Whenever new sampled architectures share bigrams (i.e., pairs of subsequent layers) with previous architectures, we want to be able to retrieve the weights of older instances of the same pair as they may help traversing the correct region of the serch space. The first time this happens, the choice of substituing newer weights with older ones is completely random and does not guarantee at all that it would result in a performance gain. As a bigram is encountered more and more often in newer architectures, it means that the controller
        has learned to sample that particular pair as a valauable component for an architecture whose weights will be trained more.

        Despite one-shot method showed good potential on NAS problem, there are still some open questions and it is unclear to what extent it may help or degrade the performance of the model.
        """
        layer_configs = ['input']

        for layer in model.layers:
            if 'flatten' in layer.name:
                layer_configs.append('flatten') 
            elif 'dropout' not in layer.name:
                layer_configs.append((layer.get_config()['units'],
                                    layer.get_config()['activation']))

        config_ids = []
        for i in range(1, len(layer_configs)):
            config_ids.append((layer_configs[i - 1], layer_configs[i]))

        j = 0
        for i, layer in enumerate(model.layers):
            if 'dropout' not in layer.name:
                bigram = config_ids[j]

                match_idx = None
                for idx, row in self.shared_weights.iterrows():
                    if row['bigram_id'] == bigram:
                        match_idx = idx
                        break

                if match_idx is not None:
                    self.shared_weights.at[match_idx, 'weights'] = layer.get_weights()
                else:
                    new_row = pd.DataFrame([{
                        'bigram_id': bigram,
                        'weights': layer.get_weights()
                    }])
                    self.shared_weights = pd.concat([self.shared_weights, new_row], ignore_index=True)

                j += 1  


    def set_model_weights(self, model):
        """
        Pre-load weights before training model if the stored bigrams matc
        """

        # By adding 'input' to the possible layer configs we artificially allow to keep the weights from input features to neurons in the first layer in dedicated manner from the rest. Indeed there is no real input layer, but it justs serves the controller recognize when a layer has been used as first layer rather than at a different position.
        
        layer_configs = ['input']
        
        # Note: Check update_weights() as we apply an equivalent procedure.
        for layer in model.layers:
            if 'flatten' in layer.name:
                layer_configs.append('flatten')
            elif 'dropout' not in layer.name:
                layer_configs.append((layer.get_config()['units'], layer.get_config()['activation']))

        config_ids = []
        for i in range(1, len(layer_configs)):
            config_ids.append((layer_configs[i - 1], layer_configs[i]))

        j = 0
        for i, layer in enumerate(model.layers):
            if 'dropout' not in layer.name:
                bigram = config_ids[j]

                match_idx = None
                for idx, row in self.shared_weights.iterrows():
                    if row['bigram_id'] == bigram:
                        match_idx = idx
                        break

                if match_idx is not None:
                    print("Transferring weights for layer:", bigram)
                    try:
                        layer.set_weights(self.shared_weights.at[match_idx, 'weights'])
                    except ValueError as e:
                        print(f"[Warning] Failed to load weights for layer {layer.name}: {e}")

                j += 1

    def log_event(self):
        """
        Save current NAS generation event to a unique file.
        """
        if not self.mlp_one_shot:
            return

        if not hasattr(time, 'time_ns'): # If python < 3.8 there's no time.time_ns()
            time.time_ns = lambda: int(time.time() * 1e9)
        
        dest = 'LOGS/event{}'.format(int(time.time_ns() // 1e9))
        while os.path.exists(dest): # Loop until the created filename is unique
            dest = 'LOGS/event{}'.format(int(time
                                 .time_ns() // 1e9))

        # Make event directory
        os.mkdir(dest)

        # Save weights to binary
        self.shared_weights.to_pickle(os.path.join(dest,
                                      self.weights_file))

        # Move everything else to event directory
        filelist = os.listdir('LOGS')
        for file in filelist:
            if os.path.isfile('LOGS/{}'.format(file)):
                shutil.move('LOGS/{}'.format(file), dest)

    def train_model(self, model, X_data, y_data, nb_epochs,
                    validation_split=0.2, callbacks=None):
        """
        Train the sampled model for a few epochs before assessing its validation accuracy. Depending on the flag self.mlp_one_shot it can discriminate on whether to load (save) weights before (after) training it or to start from scratch.

        Returns
        -------
        history : keras.History
            History of the architecture's training.
        """
        if self.mlp_one_shot:
            self.set_model_weights(model)
            history = model.fit(X_data,
                                y_data,
                                epochs=nb_epochs,
                                validation_split=validation_split,
                                callbacks=callbacks,
                                verbose=0)
            self.update_weights(model)
        else:
            history = model.fit(X_data,
                                y_data,
                                epochs=nb_epochs,
                                validation_split=validation_split,
                                callbacks=callbacks,
                                verbose=0)
        return history
